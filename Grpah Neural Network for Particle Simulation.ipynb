{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72645,"status":"ok","timestamp":1713898608639,"user":{"displayName":"Keya","userId":"10387939749915914234"},"user_tz":-330},"id":"NkW-JrRS5REX","outputId":"211e7f2e-971f-47c7-b432-2a518c702aea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FiHNXXG7HO4q"},"outputs":[],"source":["import torch\n","\n","def get_random_walk_noise_for_position_sequence(\n","        position_sequence: torch.tensor,\n","        noise_std_last_step):\n","  velocity_sequence = time_diff(position_sequence)\n","  num_velocities = velocity_sequence.shape[1]\n","  velocity_sequence_noise = torch.randn(\n","      list(velocity_sequence.shape)) * (noise_std_last_step/num_velocities**0.5)\n","  velocity_sequence_noise = torch.cumsum(velocity_sequence_noise, dim=1)\n","  position_sequence_noise = torch.cat([\n","      torch.zeros_like(velocity_sequence_noise[:, 0:1]),\n","      torch.cumsum(velocity_sequence_noise, dim=1)], dim=1)\n","\n","  return position_sequence_noise"]},{"cell_type":"code","source":["import os\n","import torch\n","# os.environ['TORCH'] = torch.version\n","# print(torch.version)\n","\n","!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n","# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install torch_geometric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJiZBgCRxrr8","executionInfo":{"status":"ok","timestamp":1713898629896,"user_tz":-330,"elapsed":18140,"user":{"displayName":"Keya","userId":"10387939749915914234"}},"outputId":"e7af9a6a-63b5-49ad-b2cb-89e69af53444"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/3.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2.9/3.4 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.5.3\n"]}]},{"cell_type":"code","source":["!pip install -q torch_scatter -f https://data.pyg.org/whl/torch-2.2.1+cu121.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-S1OuIAusNx","executionInfo":{"status":"ok","timestamp":1713898639491,"user_tz":-330,"elapsed":9603,"user":{"displayName":"Keya","userId":"10387939749915914234"}},"outputId":"646bed35-d20d-468a-f71e-a85a7e3a84ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_ihZTOoIAOs","outputId":"5aa35d9c-3fc1-4d19-bcf3-9124c05034de","executionInfo":{"status":"ok","timestamp":1713721303333,"user_tz":-330,"elapsed":736100,"user":{"displayName":"Keya","userId":"10387939749915914234"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-cluster\n","  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n","Building wheels for collected packages: torch-cluster\n","  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl size=714413 sha256=76a71814cc7ec1b95c55a0481ca7e01ce3cfe31d410059c9f636fb40581cac63\n","  Stored in directory: /root/.cache/pip/wheels/51/78/c3/536637b3cdcc3313aa5e8851a6c72b97f6a01877e68c7595e3\n","Successfully built torch-cluster\n","Installing collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.3\n","Collecting torch_geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.5.3\n"]}],"source":["# !pip install torch-cluster\n","# !pip install torch_geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erF3i9YsHdzR"},"outputs":[],"source":["from typing import List\n","import torch\n","import torch.nn as nn\n","from torch_geometric.nn import MessagePassing\n","import math\n","import torch_scatter\n","\n","class MLP(torch.nn.Module):\n","    \"\"\"Multi-Layer perceptron\"\"\"\n","    def __init__(self, input_size, hidden_size, output_size, layers, layernorm=True):\n","        super().__init__()\n","        self.layers = torch.nn.ModuleList()\n","        for i in range(layers):\n","            self.layers.append(torch.nn.Linear(\n","                input_size if i == 0 else hidden_size,\n","                output_size if i == layers - 1 else hidden_size,\n","            ))\n","            if i != layers - 1:\n","                self.layers.append(torch.nn.ReLU())\n","        if layernorm:\n","            self.layers.append(torch.nn.LayerNorm(output_size))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for layer in self.layers:\n","            if isinstance(layer, torch.nn.Linear):\n","                layer.weight.data.normal_(0, 1 / math.sqrt(layer.in_features))\n","                layer.bias.data.fill_(0)\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","def build_mlp(\n","        input_size: int,\n","        hidden_layer_sizes: List[int],\n","        output_size: int = None,\n","        output_activation: nn.Module = nn.Identity,\n","        activation: nn.Module = nn.ReLU) -> nn.Module:\n","  # Size of each layer\n","  layer_sizes = [input_size] + hidden_layer_sizes\n","  if output_size:\n","    layer_sizes.append(output_size)\n","\n","  # Number of layers\n","  nlayers = len(layer_sizes) - 1\n","\n","  # Create a list of activation functions and\n","  # set the last element to output activation function\n","  act = [activation for i in range(nlayers)]\n","  act[-1] = output_activation\n","\n","  # Create a torch sequential container\n","  mlp = nn.Sequential()\n","  for i in range(nlayers):\n","    mlp.add_module(\"NN-\" + str(i), nn.Linear(layer_sizes[i],\n","                                             layer_sizes[i + 1]))\n","    mlp.add_module(\"Act-\" + str(i), act[i]())\n","\n","  return mlp\n","\n","\n","class Encoder(nn.Module):\\\n","  def __init__(\n","          self,\n","          nnode_in_features: int,\n","          nnode_out_features: int,\n","          nedge_in_features: int,\n","          nedge_out_features: int,\n","          nmlp_layers: int,\n","          mlp_hidden_dim: int):\n","    super(Encoder, self).__init__()\n","    # Encode node features as an MLP\n","    self.node_fn = nn.Sequential(*[build_mlp(nnode_in_features,\n","                                             [mlp_hidden_dim\n","                                              for _ in range(nmlp_layers)],\n","                                             nnode_out_features),\n","                                   nn.LayerNorm(nnode_out_features)])\n","    # Encode edge features as an MLP\n","    self.edge_fn = nn.Sequential(*[build_mlp(nedge_in_features,\n","                                             [mlp_hidden_dim\n","                                              for _ in range(nmlp_layers)],\n","                                             nedge_out_features),\n","                                   nn.LayerNorm(nedge_out_features)])\n","\n","  def forward(\n","          self,\n","          x: torch.tensor,\n","          edge_features: torch.tensor):\n","    return self.node_fn(x), self.edge_fn(edge_features)\n","\n","##Multi-dimension Edge Features, Edge Feautre Embedding\n","class InteractionNetwork(MessagePassing):\n","  def __init__(\n","      self,\n","      nnode_in: int,\n","      nnode_out: int,\n","      nedge_in: int,\n","      nedge_out: int,\n","      nmlp_layers: int,\n","      mlp_hidden_dim: int,\n","  ):\n","    # Aggregate features from neighbors\n","    super(InteractionNetwork, self).__init__(aggr='add')\n","    # Node MLP\n","    self.node_fn = nn.Sequential(*[build_mlp(nnode_in + nedge_out,\n","                                             [mlp_hidden_dim\n","                                              for _ in range(nmlp_layers)],\n","                                             nnode_out),\n","                                   nn.LayerNorm(nnode_out)])\n","    # Edge MLP\n","    self.edge_fn = nn.Sequential(*[build_mlp(nnode_in + nnode_in + nedge_in,\n","                                             [mlp_hidden_dim\n","                                              for _ in range(nmlp_layers)],\n","                                             nedge_out),\n","                                   nn.LayerNorm(nedge_out)])\n","\n","  def forward(self,\n","              x: torch.tensor,\n","              edge_index: torch.tensor,\n","              edge_features: torch.tensor):\n","    # Save particle state and edge features\n","    x_residual = x\n","    edge_features_residual = edge_features\n","    x, edge_features = self.propagate(\n","        edge_index=edge_index, x=x, edge_features=edge_features)\n","\n","    return x + x_residual, edge_features + edge_features_residual\n","\n","  def message(self,\n","              x_i: torch.tensor,\n","              x_j: torch.tensor,\n","              edge_features: torch.tensor) -> torch.tensor:\n","    # Concat edge features with a final shape of [nedges, latent_dim*3]\n","    # print('hello')\n","    # print(\"x_i\", x_i.shape)\n","    # print(\"x_j\", x_j.shape)\n","\n","    # print(\"1\", edge_features.shape)\n","    edge_features = torch.cat([x_i, x_j, edge_features], dim=-1)\n","    # print(\"2\", edge_features.shape)\n","    edge_features = self.edge_fn(edge_features)\n","    # print(\"3\", edge_features.shape)\n","    return edge_features\n","\n","  def update(self,\n","             x_updated: torch.tensor,\n","             x: torch.tensor,\n","             edge_features: torch.tensor):\n","    x_updated = torch.cat([x_updated, x], dim=-1)\n","    x_updated = self.node_fn(x_updated)\n","    return x_updated, edge_features\n","\n","class Processor(MessagePassing):\n","  def __init__(\n","      self,\n","      nnode_in: int,\n","      nnode_out: int,\n","      nedge_in: int,\n","      nedge_out: int,\n","      nmessage_passing_steps: int,\n","      nmlp_layers: int,\n","      mlp_hidden_dim: int,\n","  ):\n","    super(Processor, self).__init__(aggr='max')\n","    self.gnn_stacks = nn.ModuleList([\n","        InteractionNetwork(\n","            nnode_in=nnode_in,\n","            nnode_out=nnode_out,\n","            nedge_in=nedge_in,\n","            nedge_out=nedge_out,\n","            nmlp_layers=nmlp_layers,\n","            mlp_hidden_dim=mlp_hidden_dim,\n","        ) for _ in range(nmessage_passing_steps)])\n","\n","  def forward(self,\n","              x: torch.tensor,\n","              edge_index: torch.tensor,\n","              edge_features: torch.tensor):\n","    for gnn in self.gnn_stacks:\n","      x, edge_features = gnn(x, edge_index, edge_features)\n","    return x, edge_features\n","\n","\n","class Decoder(nn.Module):\n","  def __init__(\n","          self,\n","          nnode_in: int,\n","          nnode_out: int,\n","          nmlp_layers: int,\n","          mlp_hidden_dim: int):\n","    super(Decoder, self).__init__()\n","    self.node_fn = build_mlp(\n","        nnode_in, [mlp_hidden_dim for _ in range(nmlp_layers)], nnode_out)\n","\n","  def forward(self,\n","              x: torch.tensor):\n","    return self.node_fn(x)\n","\n","\n","class EncodeProcessDecode(nn.Module):\n","  def __init__(\n","      self,\n","      nnode_in_features: int,\n","      nnode_out_features: int,\n","      nedge_in_features: int,\n","      latent_dim: int,\n","      nmessage_passing_steps: int,\n","      nmlp_layers: int,\n","      mlp_hidden_dim: int,\n","  ):\n","    super(EncodeProcessDecode, self).__init__()\n","    self._encoder = Encoder(\n","        nnode_in_features=nnode_in_features,\n","        nnode_out_features=latent_dim,\n","        nedge_in_features=nedge_in_features,\n","        nedge_out_features=latent_dim,\n","        nmlp_layers=nmlp_layers,\n","        mlp_hidden_dim=mlp_hidden_dim,\n","    )\n","    self._processor = Processor(\n","        nnode_in=latent_dim,\n","        nnode_out=latent_dim,\n","        nedge_in=latent_dim,\n","        nedge_out=latent_dim,\n","        nmessage_passing_steps=nmessage_passing_steps,\n","        nmlp_layers=nmlp_layers,\n","        mlp_hidden_dim=mlp_hidden_dim,\n","    )\n","    self._decoder = Decoder(\n","        nnode_in=latent_dim,\n","        nnode_out=nnode_out_features,\n","        nmlp_layers=nmlp_layers,\n","        mlp_hidden_dim=mlp_hidden_dim,\n","    )\n","\n","  def forward(self,\n","              x: torch.tensor,\n","              edge_index: torch.tensor,\n","              edge_features: torch.tensor):\n","    # print(\"asdfasf\")\n","    # print(x.shape)\n","    # print(edge_index.shape)\n","    # print(edge_features.shape)\n","    x, edge_features = self._encoder(x, edge_features)\n","    # print('fdgjytjydyj')\n","    # print(x.shape,edge_features.shape)\n","    x, edge_features = self._processor(x, edge_index, edge_features)\n","    # print('fdgjytjydyj')\n","    # print(x.shape,edge_features.shape)\n","    x = self._decoder(x)\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gEGl3VrBHbCa"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch_geometric.nn import radius_graph\n","from typing import Dict\n","\n","\n","class LearnedSimulator(nn.Module):\n","\n","  def __init__(\n","          self,\n","          particle_dimensions: int,\n","          nnode_in: int,\n","          nedge_in: int,\n","          latent_dim: int,\n","          nmessage_passing_steps: int,\n","          nmlp_layers: int,\n","          mlp_hidden_dim: int,\n","          connectivity_radius: float,\n","          boundaries: np.ndarray,\n","          normalization_stats: dict,\n","          nparticle_types: int,\n","          particle_type_embedding_size: int,\n","          boundary_clamp_limit: float = 1.0,\n","          device=\"cpu\"\n","  ):\n","    super(LearnedSimulator, self).__init__()\n","    self._boundaries = boundaries\n","    self._connectivity_radius = connectivity_radius\n","    self._normalization_stats = normalization_stats\n","    self._nparticle_types = nparticle_types\n","    self._boundary_clamp_limit = boundary_clamp_limit\n","\n","    # Particle type embedding has shape (9, 16)\n","    self._particle_type_embedding = nn.Embedding(\n","        nparticle_types, particle_type_embedding_size)\n","\n","    # Initialize the EncodeProcessDecode\n","    self._encode_process_decode = EncodeProcessDecode(\n","        nnode_in_features=nnode_in,\n","        nnode_out_features=particle_dimensions,\n","        nedge_in_features=nedge_in,\n","        latent_dim=latent_dim,\n","        nmessage_passing_steps=nmessage_passing_steps,\n","        nmlp_layers=nmlp_layers,\n","        mlp_hidden_dim=mlp_hidden_dim)\n","\n","    self._device = device\n","\n","  def forward(self):\n","    pass\n","\n","  def _compute_graph_connectivity(\n","          self,\n","          node_features: torch.tensor,\n","          nparticles_per_example: torch.tensor,\n","          radius: float,\n","          add_self_edges: bool = True):\n","    # Specify examples id for particles\n","    batch_ids = torch.cat(\n","        [torch.LongTensor([i for _ in range(n)])\n","         for i, n in enumerate(nparticles_per_example)]).to(self._device)\n","    # print(\"Btach_IDs:\",batch_ids.shape)\n","    # radius_graph accepts r < radius not r <= radius\n","    # A torch tensor list of source and target nodes with shape (2, nedges)\n","    edge_index = radius_graph(\n","        node_features, r=radius, batch=batch_ids, loop=add_self_edges, max_num_neighbors=128)\n","    # print(\"Edge_Index\", edge_index)\n","    # The flow direction when using in combination with message passing is\n","    # \"source_to_target\"\n","    receivers = edge_index[0, :]\n","    senders = edge_index[1, :]\n","    # print(\"Rec\", receivers.shape)\n","    # print(rec)\n","    # print(\"Sen\", senders.shape)\n","    return receivers, senders\n","\n","  def _encoder_preprocessor(\n","          self,\n","          position_sequence: torch.tensor,\n","          nparticles_per_example: torch.tensor,\n","          particle_types: torch.tensor,\n","          material_property: torch.tensor = None):\n","    nparticles = position_sequence.shape[0]\n","    # print(\"N_particles:\",nparticles)\n","    most_recent_position = position_sequence[:, -1]  # (n_nodes, 2)\n","    velocity_sequence = time_diff(position_sequence)\n","    # print(\"Velocity_sequence\",velocity_sequence.shape)\n","    # Get connectivity of the graph with shape of (nparticles, 2)\n","    senders, receivers = self._compute_graph_connectivity(\n","        most_recent_position, nparticles_per_example, self._connectivity_radius)\n","    node_features = []\n","    # print(velocity_sequence.device)\n","    # print(self._normalization_stats.device)\n","    # Normalized velocity sequence, merging spatial an time axis.\n","    velocity_stats = self._normalization_stats[\"velocity\"]\n","    normalized_velocity_sequence = (\n","        velocity_sequence - velocity_stats['mean']) / velocity_stats['std']\n","    flat_velocity_sequence = normalized_velocity_sequence.view(\n","        nparticles, -1)\n","    node_features.append(flat_velocity_sequence)\n","    # print(\"1:\")\n","    # for i in node_features:\n","    #   print(len(i))\n","    boundaries = torch.tensor(\n","        self._boundaries, requires_grad=False).float().to(self._device)\n","    # print(boundaries)\n","    # print(most_recent_position==senders)\n","    # print(most_recent_position.shape)\n","    distance_to_lower_boundary = (\n","        most_recent_position - boundaries[:, 0][None])\n","    distance_to_upper_boundary = (\n","        boundaries[:, 1][None] - most_recent_position)\n","    distance_to_boundaries = torch.cat(\n","        [distance_to_lower_boundary, distance_to_upper_boundary], dim=1)\n","    # print(\"shpape:\",distance_to_boundaries.shape)\n","    normalized_clipped_distance_to_boundaries = torch.clamp(\n","        distance_to_boundaries / self._connectivity_radius,\n","        -self._boundary_clamp_limit, self._boundary_clamp_limit)\n","    # The distance to 4 boundaries (top/bottom/left/right)\n","    # node_features shape (nparticles, 10+4)\n","    node_features.append(normalized_clipped_distance_to_boundaries)\n","    # print(\"2:\")\n","    # for i in node_features:\n","    #   print(len(i))\n","    #   print(len(i[0]))\n","    # Particle type\n","    if self._nparticle_types > 1:\n","      particle_type_embeddings = self._particle_type_embedding(\n","          particle_types)\n","      node_features.append(particle_type_embeddings)\n","\n","    # Material property\n","    if material_property is not None:\n","        material_property = material_property.view(nparticles, 1)\n","        node_features.append(material_property)\n","    # Collect edge features.\n","    edge_features = []\n","    normalized_relative_displacements = (\n","        most_recent_position[senders, :] -\n","        most_recent_position[receivers, :]\n","    ) / self._connectivity_radius\n","\n","    # print(\"1normdim\", normalized_relative_displacements.shape, len(normalized_relative_displacements[0]))\n","    edge_features.append(normalized_relative_displacements)\n","\n","    # Add relative distance between 2 particles with shape (nparticles, 1)\n","    # Edge features has a final shape of (nparticles, ndim + 1)\n","    normalized_relative_distances = torch.norm(\n","        normalized_relative_displacements, dim=-1, keepdim=True)\n","    edge_features.append(normalized_relative_distances)\n","\n","    # print(\"qwerty\", torch.stack([senders, receivers]).shape)\n","    return (torch.cat(node_features, dim=-1),\n","            torch.stack([senders, receivers]),\n","            torch.cat(edge_features, dim=-1))\n","\n","  def _decoder_postprocessor(\n","          self,\n","          normalized_acceleration: torch.tensor,\n","          position_sequence: torch.tensor) -> torch.tensor:\n","    # Extract real acceleration values from normalized values\n","    acceleration_stats = self._normalization_stats[\"acceleration\"]\n","    acceleration = (\n","        normalized_acceleration * acceleration_stats['std']\n","    ) + acceleration_stats['mean']\n","    most_recent_position = position_sequence[:, -1]\n","    most_recent_velocity = most_recent_position - position_sequence[:, -2]\n","    new_velocity = most_recent_velocity + acceleration  # * dt = 1\n","    new_position = most_recent_position + new_velocity  # * dt = 1\n","    return new_position\n","\n","  def predict_positions(\n","          self,\n","          current_positions: torch.tensor,\n","          nparticles_per_example: torch.tensor,\n","          particle_types: torch.tensor,\n","          material_property: torch.tensor = None) -> torch.tensor:\n","    if material_property is not None:\n","        node_features, edge_index, edge_features = self._encoder_preprocessor(\n","            current_positions, nparticles_per_example, particle_types, material_property)\n","    else:\n","        node_features, edge_index, edge_features = self._encoder_preprocessor(\n","            current_positions, nparticles_per_example, particle_types)\n","    predicted_normalized_acceleration = self._encode_process_decode(\n","        node_features, edge_index, edge_features)\n","    next_positions = self._decoder_postprocessor(\n","        predicted_normalized_acceleration, current_positions)\n","    # next_positions=predicted_normalized_acceleration\n","    return next_positions\n","\n","  def predict_accelerations(\n","          self,\n","          next_positions: torch.tensor,\n","          position_sequence_noise: torch.tensor,\n","          position_sequence: torch.tensor,\n","          nparticles_per_example: torch.tensor,\n","          particle_types: torch.tensor,\n","          material_property: torch.tensor = None):\n","\n","    # Add noise to the input position sequence.\n","    noisy_position_sequence = position_sequence + position_sequence_noise\n","    # print(\"noisy_posi_shape\", noisy_position_sequence.shape)\n","    # print(\"noisy\", position_sequence_noise)\n","    # print(\"noi/syshape\", position_sequence)\n","    # Perform the forward pass with the noisy position sequence.\n","    if material_property is not None:\n","        node_features, edge_index, edge_features = self._encoder_preprocessor(\n","            noisy_position_sequence, nparticles_per_example, particle_types, material_property)\n","    else:\n","        node_features, edge_index, edge_features = self._encoder_preprocessor(\n","            noisy_position_sequence, nparticles_per_example, particle_types)\n","\n","    predicted_normalized_acceleration = self._encode_process_decode(\n","        node_features, edge_index, edge_features)\n","    next_position_adjusted = next_positions + position_sequence_noise[:, -1]\n","    # print(\"next_psito\",next_position_adjusted.shape)\n","    target_normalized_acceleration = self._inverse_decoder_postprocessor(\n","        next_position_adjusted, noisy_position_sequence)\n","\n","    return predicted_normalized_acceleration, target_normalized_acceleration\n","\n","  def _inverse_decoder_postprocessor(\n","          self,\n","          next_position: torch.tensor,\n","          position_sequence: torch.tensor):\n","    previous_position = position_sequence[:, -1]\n","    previous_velocity = previous_position - position_sequence[:, -2]\n","    next_velocity = next_position - previous_position\n","    acceleration = next_velocity - previous_velocity\n","\n","    acceleration_stats = self._normalization_stats[\"acceleration\"]\n","    normalized_acceleration = (\n","        acceleration - acceleration_stats['mean']) / acceleration_stats['std']\n","    return normalized_acceleration\n","\n","  def save(\n","          self,\n","          path: str = 'model.pt'):\n","    torch.save(self.state_dict(), path)\n","\n","  def load(\n","          self,\n","          path: str):\n","    self.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","\n","\n","def time_diff(\n","        position_sequence: torch.tensor) -> torch.tensor:\n","  return position_sequence[:, 1:] - position_sequence[:, :-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwLGlDLqHW_6"},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","\n","def load_npz_data(path):\n","    with np.load(path, allow_pickle=True) as data_file:\n","        if 'gns_data' in data_file:\n","            data = data_file['gns_data']\n","        elif('arr_0' in data_file):\n","            data=data_file['arr_0']\n","        else:\n","            data = [item for _, item in data_file.items()]\n","    return data\n","\n","\n","class SamplesDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, path, input_length_sequence):\n","        super().__init__()\n","        self._data = load_npz_data(path)\n","        self._dimension = self._data[0][0].shape[-1]\n","        self._input_length_sequence = input_length_sequence\n","        self._material_property_as_feature = True if len(self._data[0]) >= 3 else False\n","        # print(len(self._data[0]))\n","        if self._material_property_as_feature:  # if raw data includes material_property\n","            self._data_lengths = [x.shape[0] - self._input_length_sequence for x, _, _ in self._data]\n","        else:\n","            self._data_lengths = [x.shape[0] - self._input_length_sequence for x, _, in self._data]\n","        self._length = sum(self._data_lengths)\n","        self._precompute_cumlengths = [sum(self._data_lengths[:x]) for x in range(1, len(self._data_lengths) + 1)]\n","        self._precompute_cumlengths = np.array(self._precompute_cumlengths, dtype=int)\n","\n","    def __len__(self):\n","        return self._length\n","\n","    def __getitem__(self, idx):\n","        # Select the trajectory immediately before\n","        # the one that exceeds the idx\n","        # (i.e., the one in which idx resides).\n","        trajectory_idx = np.searchsorted(self._precompute_cumlengths - 1, idx, side=\"left\")\n","\n","        # Compute index of pick along time-dimension of trajectory.\n","        start_of_selected_trajectory = self._precompute_cumlengths[trajectory_idx - 1] if trajectory_idx != 0 else 0\n","        time_idx = self._input_length_sequence + (idx - start_of_selected_trajectory)\n","\n","        # Prepare training data.\n","        positions = self._data[trajectory_idx][0][time_idx - self._input_length_sequence:time_idx]\n","        positions = np.transpose(positions, (1, 0, 2))  # nparticles, input_sequence_length, dimension\n","        particle_type = np.full(positions.shape[0], self._data[trajectory_idx][1], dtype=int)\n","        n_particles_per_example = positions.shape[0]\n","        label = self._data[trajectory_idx][0][time_idx]\n","\n","        if self._material_property_as_feature:  # if raw data includes material_property\n","            material_property = np.full(positions.shape[0], self._data[trajectory_idx][2], dtype=float)\n","            training_example = ((positions, particle_type, material_property, n_particles_per_example), label)\n","        else:\n","            training_example = ((positions, particle_type, n_particles_per_example), label)\n","\n","        return training_example\n","\n","\n","def collate_fn(data):\n","    material_property_as_feature = True if len(data[0][0]) >= 4 else False\n","    position_list = []\n","    particle_type_list = []\n","    if material_property_as_feature:\n","        material_property_list = []\n","    n_particles_per_example_list = []\n","    label_list = []\n","\n","    if material_property_as_feature:\n","        for ((positions, particle_type, material_property, n_particles_per_example), label) in data:\n","            position_list.append(positions)\n","            particle_type_list.append(particle_type)\n","            material_property_list.append(material_property)\n","            n_particles_per_example_list.append(n_particles_per_example)\n","            label_list.append(label)\n","    else:\n","        for ((positions, particle_type, n_particles_per_example), label) in data:\n","            position_list.append(positions)\n","            particle_type_list.append(particle_type)\n","            n_particles_per_example_list.append(n_particles_per_example)\n","            label_list.append(label)\n","\n","    if material_property_as_feature:\n","        collated_data = (\n","            (\n","                torch.tensor(np.vstack(position_list)).to(torch.float32).contiguous(),\n","                torch.tensor(np.concatenate(particle_type_list)).contiguous(),\n","                torch.tensor(np.concatenate(material_property_list)).to(torch.float32).contiguous(),\n","                torch.tensor(n_particles_per_example_list).contiguous(),\n","            ),\n","            torch.tensor(np.vstack(label_list)).to(torch.float32).contiguous()\n","        )\n","    else:\n","        collated_data = (\n","            (\n","                torch.tensor(np.vstack(position_list)).to(torch.float32).contiguous(),\n","                torch.tensor(np.concatenate(particle_type_list)).contiguous(),\n","                torch.tensor(n_particles_per_example_list).contiguous(),\n","            ),\n","            torch.tensor(np.vstack(label_list)).to(torch.float32).contiguous()\n","        )\n","\n","    return collated_data\n","\n","\n","class TrajectoriesDataset(torch.utils.data.Dataset):\n","    def __init__(self, path):\n","        super().__init__()\n","        # load dataset stored in npz format\n","        # data is loaded as dict of tuples\n","        # of the form (positions, particle_type)\n","        # convert to list of tuples\n","        # TODO (jpv): allow_pickle=True is potential security risk. See docs.\n","        self._data = load_npz_data(path)\n","        self._dimension = self._data[0][0].shape[-1]\n","        self._length = len(self._data)\n","        self._material_property_as_feature = True if len(self._data[0]) >= 3 else False\n","\n","    def __len__(self):\n","        return self._length\n","\n","    def __getitem__(self, idx):\n","        if self._material_property_as_feature:\n","            positions, _particle_type, _material_property = self._data[idx]\n","            positions = np.transpose(positions, (1, 0, 2))\n","            particle_type = np.full(positions.shape[0], _particle_type, dtype=int)\n","            material_property = np.full(positions.shape[0], _material_property, dtype=float)\n","            n_particles_per_example = positions.shape[0]\n","\n","            trajectory = (\n","                torch.tensor(positions).to(torch.float32).contiguous(),\n","                torch.tensor(particle_type).contiguous(),\n","                torch.tensor(material_property).to(torch.float32).contiguous(),\n","                n_particles_per_example\n","            )\n","        else:\n","            positions, _particle_type = self._data[idx]\n","            positions = np.transpose(positions, (1, 0, 2))\n","            particle_type = np.full(positions.shape[0], _particle_type, dtype=int)\n","            n_particles_per_example = positions.shape[0]\n","\n","            trajectory = (\n","                torch.tensor(positions).to(torch.float32).contiguous(),\n","                torch.tensor(particle_type).contiguous(),\n","                n_particles_per_example\n","            )\n","\n","        return trajectory\n","\n","\n","def get_data_loader_by_samples(path, input_length_sequence, batch_size, shuffle=False):\n","    dataset = SamplesDataset(path, input_length_sequence)\n","    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n","                                       pin_memory=True, collate_fn=collate_fn)\n","\n","\n","def get_data_loader_by_trajectories(path):\n","    dataset = TrajectoriesDataset(path)\n","    return torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=False,\n","                                       pin_memory=True)"]},{"cell_type":"markdown","metadata":{"id":"uC19hB3EL66q"},"source":["#READING_UTILS.PY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXjknSskL6fk"},"outputs":[],"source":["import os\n","import json\n","\n","def read_metadata(data_path: str,\n","                  purpose: str,\n","                  file_name: str = \"metadata.json\"):\n","  try:\n","    with open(os.path.join(data_path, file_name), 'rt') as fp:\n","      # New version use separate metadata for `train` and `rollout`.\n","      metadata = json.loads(fp.read())[purpose]\n","\n","  except:\n","    with open(os.path.join(data_path, file_name), 'rt') as fp:\n","      # The previous format of the metadata does not distinguish the purpose of metadata\n","      metadata = json.loads(fp.read())\n","\n","  return metadata\n","\n","def flags_to_dict(FLAGS):\n","  flags_dict = {}\n","  for name in FLAGS:\n","    flag_value = FLAGS[name].value\n","    flags_dict[name] = flag_value\n","  return flags_dict"]},{"cell_type":"markdown","metadata":{"id":"QrnsexevLvVM"},"source":["#TRAIN.PY\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDzw4xhVSLSJ"},"outputs":[],"source":["def del_all_flags(FLAGS):\n","    flags_dict = FLAGS._flags()\n","    keys_list = [keys for keys in flags_dict]\n","    for keys in keys_list:\n","        FLAGS.__delattr__(keys)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":474},"id":"HdHup5YvG7Lp","outputId":"4dcdf0ef-886f-4ccd-e4c2-89a541ae2860","executionInfo":{"status":"error","timestamp":1713901627869,"user_tz":-330,"elapsed":624058,"user":{"displayName":"Keya","userId":"10387939749915914234"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/models/model-106500_num_1_timestamp_100.pt\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 219/219 [04:17<00:00,  1.17s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Predicting example 0 loss: 0.010417882353067398\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 219/219 [03:31<00:00,  1.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Predicting example 1 loss: 0.0032257132697850466\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 219/219 [02:30<00:00,  1.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Predicting example 2 loss: 0.012547919526696205\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 4/219 [00:02<02:02,  1.76it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-0c52df8a71a6>\u001b[0m in \u001b[0;36m<cell line: 535>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-44-0c52df8a71a6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_device_number\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'cuda:{int(FLAGS.cuda_device_number)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-0c52df8a71a6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;31m# Predict example rollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m       example_rollout, loss = rollout(simulator,\n\u001b[0m\u001b[1;32m    165\u001b[0m                                       \u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                                       \u001b[0mparticle_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-0c52df8a71a6>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(simulator, position, particle_types, material_property, n_particles_per_example, nsteps, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Get next position with shape (nnodes, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     next_position = simulator.predict_positions(\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mcurrent_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mnparticles_per_example\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_particles_per_example\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-9cf165598b0f>\u001b[0m in \u001b[0;36mpredict_positions\u001b[0;34m(self, current_positions, nparticles_per_example, particle_types, material_property)\u001b[0m\n\u001b[1;32m    277\u001b[0m         node_features, edge_index, edge_features = self._encoder_preprocessor(\n\u001b[1;32m    278\u001b[0m             current_positions, nparticles_per_example, particle_types)\n\u001b[0;32m--> 279\u001b[0;31m     predicted_normalized_acceleration = self._encode_process_decode(\n\u001b[0m\u001b[1;32m    280\u001b[0m         node_features, edge_index, edge_features)\n\u001b[1;32m    281\u001b[0m     next_positions = self._decoder_postprocessor(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-92c947e16e08>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_features)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;31m# print('fdgjytjydyj')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;31m# print(x.shape,edge_features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m     \u001b[0;31m# print('fdgjytjydyj')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;31m# print(x.shape,edge_features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-92c947e16e08>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_features)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \"\"\"\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn_stacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-92c947e16e08>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_features)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# Takes in the edge indices and all additional data which is needed to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# construct messages and to update node embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     x, edge_features = self.propagate(\n\u001b[0m\u001b[1;32m    202\u001b[0m         edge_index=edge_index, x=x, edge_features=edge_features)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                         \u001b[0mmsg_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-92c947e16e08>\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, x_i, x_j, edge_features)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0medge_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# print(\"2\", edge_features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0medge_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;31m# print(\"3\", edge_features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0medge_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import collections\n","import json\n","import os\n","import pickle\n","import glob\n","import re\n","import gc\n","import sys\n","import numpy as np\n","import torch\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from tqdm import tqdm\n","from absl import flags\n","from absl import app\n","\n","del_all_flags(flags.FLAGS)\n","# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n","# del flags\n","flags.DEFINE_enum(\n","    'mode', 'rollout', ['train', 'valid', 'rollout'],\n","    help='Train model, validation or rollout evaluation.')\n","flags.DEFINE_integer('batch_size', 2, help='The batch size.')\n","flags.DEFINE_float('noise_std', 6.7e-4, help='The std deviation of the noise.')\n","flags.DEFINE_string('data_path','/content/drive/MyDrive/', help='The dataset directory.')\n","# flags.DEFINE_string('data_path','/content/', help='The dataset directory.')\n","flags.DEFINE_string('model_path', '/content/drive/MyDrive/models/', help=('The path for saving checkpoints of the model.'))\n","flags.DEFINE_string('output_path', '/content/drive/MyDrive/rollouts-106500_num_1_timestamp_100/test/', help='The path for saving outputs (e.g. rollouts).')\n","flags.DEFINE_string('output_filename', 'rollout', help='Base name for saving the rollout')\n","flags.DEFINE_string('model_file', 'model-106500_num_1_timestamp_100.pt' , help=('Model filename (.pt) to resume from. Can also use \"latest\" to default to newest file.'))\n","flags.DEFINE_string('train_state_file', 'train_state.pt', help=('Train state filename (.pt) to resume from. Can also use \"latest\" to default to newest file.'))\n","\n","flags.DEFINE_integer('ntraining_steps', int(2E7), help='Number of training steps.')\n","flags.DEFINE_integer('nsave_steps', int(1000), help='Number of steps at which to save the model.')\n","\n","# Learning rate parameters\n","flags.DEFINE_float('lr_init', 1e-2, help='Initial learning rate.')\n","flags.DEFINE_float('lr_decay', 0.1, help='Learning rate decay.')\n","flags.DEFINE_integer('lr_decay_steps', int(5e6), help='Learning rate decay steps.')\n","\n","flags.DEFINE_integer(\"cuda_device_number\", None, help=\"CUDA device (zero indexed), default is None so default CUDA device will be used.\")\n","flags.DEFINE_integer(\"n_gpus\", 1, help=\"The number of GPUs to utilize for training\")\n","\n","FLAGS = flags.FLAGS\n","FLAGS(sys.argv[1:])\n","\n","Stats = collections.namedtuple('Stats', ['mean', 'std'])\n","\n","INPUT_SEQUENCE_LENGTH = 101  # So we can calculate the last 5 velocities.\n","NUM_PARTICLE_TYPES = 1\n","KINEMATIC_PARTICLE_ID = 3\n","\n","def rollout(\n","        simulator: LearnedSimulator,\n","        position: torch.tensor,\n","        particle_types: torch.tensor,\n","        material_property: torch.tensor,\n","        n_particles_per_example: torch.tensor,\n","        nsteps: int,\n","        device: torch.device):\n","\n","  initial_positions = position[:, :INPUT_SEQUENCE_LENGTH]\n","  ground_truth_positions = position[:, INPUT_SEQUENCE_LENGTH:]\n","\n","  current_positions = initial_positions\n","  predictions = []\n","\n","  for step in tqdm(range(nsteps), total=nsteps):\n","    # Get next position with shape (nnodes, dim)\n","    next_position = simulator.predict_positions(\n","        current_positions,\n","        nparticles_per_example=[n_particles_per_example],\n","        particle_types=particle_types,\n","        material_property=material_property\n","    )\n","\n","    # Update kinematic particles from prescribed trajectory.\n","    kinematic_mask = (particle_types == KINEMATIC_PARTICLE_ID).clone().detach().to(device)\n","    next_position_ground_truth = ground_truth_positions[:, step]\n","    kinematic_mask = kinematic_mask.bool()[:, None].expand(-1, current_positions.shape[-1])\n","    next_position = torch.where(\n","        kinematic_mask, next_position_ground_truth, next_position)\n","    predictions.append(next_position)\n","\n","    # Shift `current_positions`, removing the oldest position in the sequence\n","    # and appending the next position at the end.\n","    current_positions = torch.cat(\n","        [current_positions[:, 1:], next_position[:, None, :]], dim=1)\n","\n","  # Predictions with shape (time, nnodes, dim)\n","  predictions = torch.stack(predictions)\n","  ground_truth_positions = ground_truth_positions.permute(1, 0, 2)\n","\n","  loss = (predictions - ground_truth_positions) ** 2\n","\n","  output_dict = {\n","      'initial_positions': initial_positions.permute(1, 0, 2).cpu().numpy(),\n","      'predicted_rollout': predictions.cpu().numpy(),\n","      'ground_truth_rollout': ground_truth_positions.cpu().numpy(),\n","      'particle_types': particle_types.cpu().numpy(),\n","      'material_property': material_property.cpu().numpy() if material_property is not None else None\n","  }\n","\n","  return output_dict, loss\n","\n","\n","def predict(device: str):\n","  \"\"\"Predict rollouts.\n","\n","  Args:\n","    simulator: Trained simulator if not will undergo training.\n","\n","  \"\"\"\n","  # Read metadata\n","  metadata = read_metadata(FLAGS.data_path, \"rollout\",\"metadata_sand.json\")\n","  simulator = _get_simulator(metadata, FLAGS.noise_std, FLAGS.noise_std, device)\n","\n","  # Load simulator\n","  print(FLAGS.model_path + FLAGS.model_file)\n","  if os.path.exists(FLAGS.model_path + FLAGS.model_file):\n","    simulator.load(FLAGS.model_path + FLAGS.model_file)\n","  else:\n","    raise Exception(f\"Model does not exist at {FLAGS.model_path + FLAGS.model_file}\")\n","\n","  simulator.to(device)\n","  simulator.eval()\n","\n","  # Output path\n","  if not os.path.exists(FLAGS.output_path):\n","    os.makedirs(FLAGS.output_path)\n","\n","  # Use `valid`` set for eval mode if not use `test`\n","  split = 'test' if FLAGS.mode == 'rollout' else 'valid'\n","\n","  # Get dataset\n","  ds = get_data_loader_by_trajectories(path=f\"{FLAGS.data_path}{split}.npz\")\n","  # See if our dataset has material property as feature\n","  if len(ds.dataset._data[0]) == 3:  # `ds` has (positions, particle_type, material_property)\n","    material_property_as_feature = True\n","  elif len(ds.dataset._data[0]) == 2:  # `ds` only has (positions, particle_type)\n","    material_property_as_feature = False\n","  else:\n","    raise NotImplementedError\n","\n","  eval_loss = []\n","  with torch.no_grad():\n","    for example_i, features in enumerate(ds):\n","      positions = features[0].to(device)\n","      if metadata['sequence_length'] is not None:\n","        # If `sequence_length` is predefined in metadata,\n","        nsteps = metadata['sequence_length'] - INPUT_SEQUENCE_LENGTH\n","      else:\n","        # If no predefined `sequence_length`, then get the sequence length\n","        sequence_length = positions.shape[1]\n","        nsteps = sequence_length - INPUT_SEQUENCE_LENGTH\n","      particle_type = features[1].to(device)\n","      if material_property_as_feature:\n","        material_property = features[2].to(device)\n","        n_particles_per_example = torch.tensor([int(features[3])], dtype=torch.int32).to(device)\n","      else:\n","        material_property = None\n","        n_particles_per_example = torch.tensor([int(features[2])], dtype=torch.int32).to(device)\n","\n","      # Predict example rollout\n","      example_rollout, loss = rollout(simulator,\n","                                      positions,\n","                                      particle_type,\n","                                      material_property,\n","                                      n_particles_per_example,\n","                                      nsteps,\n","                                      device)\n","\n","      example_rollout['metadata'] = metadata\n","      print(\"Predicting example {} loss: {}\".format(example_i, loss.mean()))\n","      eval_loss.append(torch.flatten(loss))\n","\n","      # Save rollout in testing\n","      if FLAGS.mode == 'rollout':\n","        example_rollout['metadata'] = metadata\n","        example_rollout['loss'] = loss.mean()\n","        filename = f'{FLAGS.output_filename}_ex{example_i}.pkl'\n","        filename = os.path.join(FLAGS.output_path, filename)\n","        with open(filename, 'wb') as f:\n","          pickle.dump(example_rollout, f)\n","\n","  print(\"Mean loss on rollout prediction: {}\".format(\n","      torch.mean(torch.cat(eval_loss))))\n","\n","def optimizer_to(optim, device):\n","  for param in optim.state.values():\n","    # Not sure there are any global tensors in the state dict\n","    if isinstance(param, torch.Tensor):\n","      param.data = param.data.to(device)\n","      if param._grad is not None:\n","        param._grad.data = param._grad.data.to(device)\n","    elif isinstance(param, dict):\n","      for subparam in param.values():\n","        if isinstance(subparam, torch.Tensor):\n","          subparam.data = subparam.data.to(device)\n","          if subparam._grad is not None:\n","            subparam._grad.data = subparam._grad.data.to(device)\n","\n","def train(rank, flags, world_size, device):\n","  temp_device='cpu'\n","  if device == torch.device(\"cuda\"):\n","    setup(rank, world_size, device)\n","    device_id = rank\n","  else:\n","    device_id = device\n","  # Read metadata\n","  metadata = read_metadata(flags[\"data_path\"], \"train\",'metadata_sand.json')\n","\n","  # # Get simulator and optimizer\n","  if device == torch.device(\"cuda\"):\n","    serial_simulator = _get_simulator(metadata, flags[\"noise_std\"], flags[\"noise_std\"], rank)\n","    simulator = DDP(serial_simulator.to(rank), device_ids=[rank], output_device=rank)\n","    optimizer = torch.optim.Adam(simulator.parameters(), lr=flags[\"lr_init\"]*world_size)\n","  else:\n","    simulator = _get_simulator(metadata, flags[\"noise_std\"], flags[\"noise_std\"], temp_device)\n","    simulator = _get_simulator(metadata, flags[\"noise_std\"], flags[\"noise_std\"], device)\n","    optimizer = torch.optim.Adam(simulator.parameters(), lr=flags[\"lr_init\"] * world_size)\n","    step = 0\n","    pass\n","\n","  # If model_path does exist and model_file and train_state_file exist continue training.\n","  if flags[\"model_file\"] is not None:\n","\n","    if flags[\"model_file\"] == \"latest\" and flags[\"train_state_file\"] == \"latest\":\n","      # find the latest model, assumes model and train_state files are in step.\n","      fnames = glob.glob(f'{flags[\"model_path\"]}*model*pt')\n","      max_model_number = 0\n","      expr = re.compile(\".*model-(\\d+).pt\")\n","      for fname in fnames:\n","        model_num = int(expr.search(fname).groups()[0])\n","        if model_num > max_model_number:\n","          max_model_number = model_num\n","      # reset names to point to the latest.\n","      flags[\"model_file\"] = f\"model-{max_model_number}.pt\"\n","      flags[\"train_state_file\"] = f\"train_state-{max_model_number}.pt\"\n","\n","    if os.path.exists(flags[\"model_path\"] + flags[\"model_file\"]) and os.path.exists(flags[\"model_path\"] + flags[\"train_state_file\"]):\n","      # load model\n","      if device == torch.device(\"cuda\"):\n","        simulator.module.load(flags[\"model_path\"] + flags[\"model_file\"])\n","      else:\n","        simulator.load(flags[\"model_path\"] + flags[\"model_file\"])\n","\n","      # load train state\n","      train_state = torch.load(flags[\"model_path\"] + flags[\"train_state_file\"])\n","      # set optimizer state\n","      optimizer = torch.optim.Adam(\n","        simulator.module.parameters() if device == torch.device(\"cuda\") else simulator.parameters())\n","      optimizer.load_state_dict(train_state[\"optimizer_state\"])\n","      optimizer_to(optimizer, device_id)\n","      # set global train state\n","      step = train_state[\"global_train_state\"].pop(\"step\")\n","\n","    else:\n","      msg = f'Specified model_file {flags[\"model_path\"] + flags[\"model_file\"]} and train_state_file {flags[\"model_path\"] + flags[\"train_state_file\"]} not found.'\n","      raise FileNotFoundError(msg)\n","\n","  simulator=simulator.to(temp_device)\n","  # simulator.to(device_id)\n","  simulator.train()\n","\n","  if device == torch.device(\"cuda\"):\n","    dl = get_data_distributed_dataloader_by_samples(path=f'{flags[\"data_path\"]}sand_train.npz',\n","                                                               input_length_sequence=INPUT_SEQUENCE_LENGTH,\n","                                                               batch_size=flags[\"batch_size\"])\n","    # dl = get_data_distributed_dataloader_by_samples(path=f'{flags[\"data_path\"]}train.npz',\n","    #                                                            input_length_sequence=INPUT_SEQUENCE_LENGTH,\n","    #                                                            batch_size=flags[\"batch_size\"])\n","  else:\n","    dl = get_data_loader_by_samples(path=f'{flags[\"data_path\"]}sand_train.npz',\n","                                                input_length_sequence=INPUT_SEQUENCE_LENGTH,\n","                                                batch_size=flags[\"batch_size\"])\n","  n_features = len(dl.dataset._data[0])\n","\n","  print(f\"rank = {rank}, cuda = {torch.cuda.is_available()}\")\n","  not_reached_nsteps = True\n","  ctr=0\n","  try:\n","    while not_reached_nsteps:\n","      if device == torch.device(\"cuda\"):\n","        torch.distributed.barrier()\n","      else:\n","        pass\n","      for example in dl:  # ((position, particle_type, material_property, n_particles_per_example), labels) are in dl\n","        # print(len(example))\n","        # print(example[0][0].shape)\n","        # for i in example:\n","          # print(i)\n","          # print('hahahah')\n","        position = example[0][0].to(temp_device)\n","        # position = example[0][0].to(device_id)\n","        # print(position[0].shape)\n","        particle_type = example[0][1].to(temp_device)\n","        # particle_type = example[0][1].to(device_id)\n","        if n_features == 3:  # if dl includes material_property\n","          material_property = example[0][2].to(temp_device)\n","          # material_property = example[0][2].to(device_id)\n","          n_particles_per_example = example[0][3].to(temp_device)\n","          # n_particles_per_example = example[0][3].to(device_id)\n","        elif n_features == 2:\n","          n_particles_per_example = example[0][2].to(temp_device)\n","          # n_particles_per_example = example[0][2].to(device_id)\n","        else:\n","          raise NotImplementedError\n","        labels = example[1].to(temp_device)\n","        # labels = example[1].to(device_id)\n","\n","        n_particles_per_example.to(temp_device)\n","        # n_particles_per_example.to(device_id)\n","        # labels.to(device_id)\n","        # print(n_particles_per_example.shape)\n","        # print(labels.shape)\n","        # TODO (jpv): Move noise addition to data_loader\n","        # Sample the noise to add to the inputs to the model during training.\n","        sampled_noise = get_random_walk_noise_for_position_sequence(position, noise_std_last_step=flags[\"noise_std\"]).to(temp_device)\n","        # sampled_noise = get_random_walk_noise_for_position_sequence(position, noise_std_last_step=flags[\"noise_std\"]).to(device_id)\n","        non_kinematic_mask = (particle_type != KINEMATIC_PARTICLE_ID).clone().detach().to(temp_device)\n","        # non_kinematic_mask = (particle_type != KINEMATIC_PARTICLE_ID).clone().detach().to(device_id)\n","        sampled_noise *= non_kinematic_mask.view(-1, 1, 1)\n","        # print(sampled_noise)\n","\n","        # Get the predictions and target accelerations.\n","        if device == torch.device(\"cuda\"):\n","          pred_acc, target_acc = simulator.module.predict_accelerations(\n","            next_positions=labels.to(rank),\n","            position_sequence_noise=sampled_noise.to(rank),\n","            position_sequence=position.to(rank),\n","            nparticles_per_example=n_particles_per_example.to(rank),\n","            particle_types=particle_type.to(rank),\n","            material_property=material_property.to(rank) if n_features == 3 else None\n","          )\n","        else:\n","          pred_acc, target_acc = simulator.predict_accelerations(\n","            next_positions=labels.to(temp_device),\n","            position_sequence_noise=sampled_noise.to(temp_device),\n","            position_sequence=position.to(temp_device),\n","            nparticles_per_example=n_particles_per_example.to(temp_device),\n","            particle_types=particle_type.to(temp_device),\n","            material_property=material_property.to(rank) if n_features == 3 else None\n","          )\n","          target_acc=target_acc.to(temp_device)\n","          # pred_acc, target_acc = simulator.predict_accelerations(\n","          #   next_positions=labels.to(device),\n","          #   position_sequence_noise=sampled_noise.to(device),\n","          #   position_sequence=position.to(device),\n","          #   nparticles_per_example=n_particles_per_example.to(device),\n","          #   particle_types=particle_type.to(device),\n","          #   material_property=material_property.to(rank) if n_features == 3 else None\n","          # )\n","\n","        # Calculate the loss and mask out loss on kinematic particles\n","        # print(pred)\n","        loss = (pred_acc - target_acc) ** 2\n","        # print(\"Loss:\",loss.shape)\n","        loss = loss.sum(dim=-1)\n","        # print(\"Loss2:\",loss.shape)\n","        # print('1',loss)\n","        num_non_kinematic = non_kinematic_mask.sum()\n","        # print(\"num_non_kinematic\", num_non_kinematic)\n","        # print(\"non_kinematic_mask\", non_kinematic_mask)\n","        loss = torch.where(non_kinematic_mask.bool(),\n","                         loss, torch.zeros_like(loss))\n","        # print('2',loss)\n","        loss = loss.sum() / num_non_kinematic\n","        # Computes the gradient of loss\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update learning rate\n","        lr_new = flags[\"lr_init\"] * (flags[\"lr_decay\"] ** (step/flags[\"lr_decay_steps\"])) * world_size\n","        for param in optimizer.param_groups:\n","          param['lr'] = lr_new\n","\n","        if True:\n","        # if rank == 0 or device == torch.device(\"cpu\"):\n","          print(f'Training step: {step}/{flags[\"ntraining_steps\"]}. Loss: {loss}.')\n","          # Save model state\n","          if step % flags[\"nsave_steps\"] == 0:\n","            if True:\n","            # if device == torch.device(\"cpu\"):\n","              simulator.save(flags[\"model_path\"] + 'model-'+str(step)+'.pt')\n","            else:\n","              simulator.module.save(flags[\"model_path\"] + 'model-'+str(step)+'.pt')\n","            train_state = dict(optimizer_state=optimizer.state_dict(),\n","                               global_train_state={\"step\": step},\n","                               loss=loss.item())\n","            torch.save(train_state, f'{flags[\"model_path\"]}train_state-{step}.pt')\n","\n","        # Complete training\n","        if (step >= flags[\"ntraining_steps\"]):\n","          not_reached_nsteps = False\n","          break\n","        step += 1\n","        print(\"Loss : \",loss.item())\n","        del position,example,labels,target_acc,pred_acc,particle_type,n_particles_per_example,loss\n","        # torch.cuda.empty_cache()\n","        gc.collect()\n","        print(\"Current training step : \",step)\n","        # ctr+=1\n","        # if(ctr==1):\n","        #   not_reached_nsteps=False\n","        #   break\n","        # print(\"NEXT\")\n","        # continue\n","  except KeyboardInterrupt:\n","    pass\n","\n","  if True:\n","  # if rank == 0 or device == torch.device(\"cpu\"):\n","    if True:\n","    # if device == torch.device(\"cpu\"):\n","      simulator.save(flags[\"model_path\"] + 'model-'+str(step)+'.pt')\n","    else:\n","      simulator.module.save(flags[\"model_path\"] + 'model-'+str(step)+'.pt')\n","    train_state = dict(optimizer_state=optimizer.state_dict(),\n","                       global_train_state={\"step\": step},\n","                       loss=loss.item())\n","    torch.save(train_state, f'{flags[\"model_path\"]}train_state-{step}.pt')\n","\n","  if torch.cuda.is_available():\n","    distribute.cleanup()\n","\n","\n","def _get_simulator(\n","        metadata: json,\n","        acc_noise_std: float,\n","        vel_noise_std: float,\n","        device: torch.device) -> LearnedSimulator:\n","  \"\"\"Instantiates the simulator.\n","\n","  Args:\n","    metadata: JSON object with metadata.\n","    acc_noise_std: Acceleration noise std deviation.\n","    vel_noise_std: Velocity noise std deviation.\n","    device: PyTorch device 'cpu' or 'cuda'.\n","  \"\"\"\n","\n","  # Normalization stats\n","  normalization_stats = {\n","      'acceleration': {\n","          'mean': torch.FloatTensor(metadata['acc_mean']).to(device),\n","          'std': torch.sqrt(torch.FloatTensor(metadata['acc_std'])**2 +\n","                            acc_noise_std**2).to(device),\n","      },\n","      'velocity': {\n","          'mean': torch.FloatTensor(metadata['vel_mean']).to(device),\n","          'std': torch.sqrt(torch.FloatTensor(metadata['vel_std'])**2 +\n","                            vel_noise_std**2).to(device),\n","      },\n","  }\n","\n","  # Get necessary parameters for loading simulator.\n","  if \"nnode_in\" in metadata and \"nedge_in\" in metadata:\n","    nnode_in = metadata['nnode_in']\n","    nedge_in = metadata['nedge_in']\n","  else:\n","    # Given that there is no additional node feature (e.g., material_property) except for:\n","    # (position (dim), velocity (dim*6), particle_type (6)),\n","    nnode_in = 37 if metadata['dim'] == 3 else 204\n","    nedge_in = metadata['dim'] + 1\n","\n","  # Init simulator.\n","  simulator = LearnedSimulator(\n","      particle_dimensions=metadata['dim'],\n","      nnode_in=nnode_in,\n","      nedge_in=nedge_in,\n","      latent_dim=128,\n","      nmessage_passing_steps=10,\n","      nmlp_layers=2,\n","      mlp_hidden_dim=128,\n","      connectivity_radius=metadata['default_connectivity_radius'],\n","      boundaries=np.array(metadata['bounds']),\n","      normalization_stats=normalization_stats,\n","      nparticle_types=NUM_PARTICLE_TYPES,\n","      particle_type_embedding_size=16,\n","      boundary_clamp_limit=metadata[\"boundary_augment\"] if \"boundary_augment\" in metadata else 1.0,\n","      device=device)\n","\n","  return simulator\n","\n","\n","def main():\n","  \"\"\"Train or evaluates the model.\n","\n","  \"\"\"\n","  # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  # if device == torch.device('cuda'):\n","  #   os.environ[\"MASTER_ADDR\"] = \"localhost\"\n","  #   os.environ[\"MASTER_PORT\"] = \"29500\"\n","  device='cpu'\n","\n","  myflags = flags_to_dict(FLAGS)\n","\n","  if FLAGS.mode == 'train':\n","    # If model_path does not exist create new directory.\n","    if not os.path.exists(FLAGS.model_path):\n","      os.makedirs(FLAGS.model_path)\n","\n","    # Train on gpu\n","    if device == torch.device('cuda'):\n","      available_gpus = torch.cuda.device_count()\n","      print(f\"Available GPUs = {available_gpus}\")\n","\n","      # Set the number of GPUs based on availability and the specified number\n","      if FLAGS.n_gpus is None or FLAGS.n_gpus > available_gpus:\n","        world_size = available_gpus\n","        if FLAGS.n_gpus is not None:\n","          print(f\"Warning: The number of GPUs specified ({FLAGS.n_gpus}) exceeds the available GPUs ({available_gpus})\")\n","      else:\n","        world_size = FLAGS.n_gpus\n","\n","      # Print the status of GPU usage\n","      print(f\"Using {world_size}/{available_gpus} GPUs\")\n","\n","      # Spawn training to GPUs\n","      spawn_train(train, myflags, world_size, device)\n","\n","    # Train on cpu\n","    else:\n","      rank = None\n","      world_size = 1\n","      train(rank, myflags, world_size, device)\n","\n","  elif FLAGS.mode in ['valid', 'rollout']:\n","    # Set device\n","    world_size = torch.cuda.device_count()\n","    if FLAGS.cuda_device_number is not None and torch.cuda.is_available():\n","      device = torch.device(f'cuda:{int(FLAGS.cuda_device_number)}')\n","    predict(device)\n","\n","if __name__ == '__main__':\n","  main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"seX7nsF1Umhl"},"outputs":[],"source":["# data=load_npz_data('train.npz')\n","# print(data[0][0].shape)\n","# print(len(data))\n","# print(data[1][0].shape)\n","# # count=0\n","# # for i in data:\n","# #   print(i)\n","# #   if(count==3):\n","# #     break\n","# #   count+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7415,"status":"ok","timestamp":1713900352996,"user":{"displayName":"Keya","userId":"10387939749915914234"},"user_tz":-330},"id":"SSfwhIgjL9fB","outputId":"884a68b4-2643-4b12-a36a-e38e6ffde751"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyevtk\n","  Downloading pyevtk-1.6.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from pyevtk) (1.25.2)\n","Installing collected packages: pyevtk\n","Successfully installed pyevtk-1.6.0\n"]}],"source":["!pip install pyevtk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22011,"status":"ok","timestamp":1713900374995,"user":{"displayName":"Keya","userId":"10387939749915914234"},"user_tz":-330},"id":"v3dnqDepOhCw","outputId":"dd4d74fa-8448-4491-be8a-c539127b6701"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript gsfonts imagemagick-6-common\n","  imagemagick-6.q16 libdjvulibre-text libdjvulibre21 libfftw3-double3 libgs9 libgs9-common libidn12\n","  libijs-0.35 libjbig2dec0 libjxr-tools libjxr0 liblqr-1-0 libmagickcore-6.q16-6\n","  libmagickcore-6.q16-6-extra libmagickwand-6.q16-6 libnetpbm10 libwmflite-0.2-7 netpbm\n","  poppler-data\n","Suggested packages:\n","  fonts-noto fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre ghostscript-x imagemagick-doc\n","  autotrace cups-bsd | lpr | lprng enscript gimp gnuplot grads hp2xx html2ps libwmf-bin mplayer\n","  povray radiance sane-utils texlive-base-bin transfig ufraw-batch libfftw3-bin libfftw3-dev\n","  inkscape poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic\n","  | fonts-ipafont-gothic fonts-arphic-ukai fonts-arphic-uming fonts-nanum\n","The following NEW packages will be installed:\n","  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript gsfonts imagemagick\n","  imagemagick-6-common imagemagick-6.q16 libdjvulibre-text libdjvulibre21 libfftw3-double3 libgs9\n","  libgs9-common libidn12 libijs-0.35 libjbig2dec0 libjxr-tools libjxr0 liblqr-1-0\n","  libmagickcore-6.q16-6 libmagickcore-6.q16-6-extra libmagickwand-6.q16-6 libnetpbm10\n","  libwmflite-0.2-7 netpbm poppler-data\n","0 upgraded, 26 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 25.1 MB of archives.\n","After this operation, 87.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfftw3-double3 amd64 3.3.8-2ubuntu8 [770 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblqr-1-0 amd64 0.4.2-2.1 [27.7 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 imagemagick-6-common all 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3 [63.6 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickcore-6.q16-6 amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3 [1,788 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickwand-6.q16-6 amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3 [328 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.6 [751 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.6 [5,031 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ghostscript amd64 9.55.0~dfsg1-0ubuntu5.6 [49.4 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.5 [3,120 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 imagemagick-6.q16 amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3 [224 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 imagemagick amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3 [14.6 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdjvulibre-text all 3.5.28-2build2 [50.9 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdjvulibre21 amd64 3.5.28-2build2 [624 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjxr0 amd64 1.2~git20170615.f752187-5 [174 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjxr-tools amd64 1.2~git20170615.f752187-5 [16.0 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwmflite-0.2-7 amd64 0.2.12-5ubuntu1 [68.9 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickcore-6.q16-6-extra amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3 [70.1 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libnetpbm10 amd64 2:10.0-15.4 [59.1 kB]\n","Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 netpbm amd64 2:10.0-15.4 [1,007 kB]\n","Fetched 25.1 MB in 5s (5,251 kB/s)\n","Selecting previously unselected package fonts-droid-fallback.\n","(Reading database ... 121752 files and directories currently installed.)\n","Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n","Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n","Selecting previously unselected package libfftw3-double3:amd64.\n","Preparing to unpack .../01-libfftw3-double3_3.3.8-2ubuntu8_amd64.deb ...\n","Unpacking libfftw3-double3:amd64 (3.3.8-2ubuntu8) ...\n","Selecting previously unselected package liblqr-1-0:amd64.\n","Preparing to unpack .../02-liblqr-1-0_0.4.2-2.1_amd64.deb ...\n","Unpacking liblqr-1-0:amd64 (0.4.2-2.1) ...\n","Selecting previously unselected package imagemagick-6-common.\n","Preparing to unpack .../03-imagemagick-6-common_8%3a6.9.11.60+dfsg-1.3ubuntu0.22.04.3_all.deb ...\n","Unpacking imagemagick-6-common (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Selecting previously unselected package libmagickcore-6.q16-6:amd64.\n","Preparing to unpack .../04-libmagickcore-6.q16-6_8%3a6.9.11.60+dfsg-1.3ubuntu0.22.04.3_amd64.deb ...\n","Unpacking libmagickcore-6.q16-6:amd64 (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Selecting previously unselected package libmagickwand-6.q16-6:amd64.\n","Preparing to unpack .../05-libmagickwand-6.q16-6_8%3a6.9.11.60+dfsg-1.3ubuntu0.22.04.3_amd64.deb ...\n","Unpacking libmagickwand-6.q16-6:amd64 (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Selecting previously unselected package poppler-data.\n","Preparing to unpack .../06-poppler-data_0.4.11-1_all.deb ...\n","Unpacking poppler-data (0.4.11-1) ...\n","Selecting previously unselected package fonts-noto-mono.\n","Preparing to unpack .../07-fonts-noto-mono_20201225-1build1_all.deb ...\n","Unpacking fonts-noto-mono (20201225-1build1) ...\n","Selecting previously unselected package fonts-urw-base35.\n","Preparing to unpack .../08-fonts-urw-base35_20200910-1_all.deb ...\n","Unpacking fonts-urw-base35 (20200910-1) ...\n","Selecting previously unselected package libgs9-common.\n","Preparing to unpack .../09-libgs9-common_9.55.0~dfsg1-0ubuntu5.6_all.deb ...\n","Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.6) ...\n","Selecting previously unselected package libidn12:amd64.\n","Preparing to unpack .../10-libidn12_1.38-4ubuntu1_amd64.deb ...\n","Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n","Selecting previously unselected package libijs-0.35:amd64.\n","Preparing to unpack .../11-libijs-0.35_0.35-15build2_amd64.deb ...\n","Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n","Selecting previously unselected package libjbig2dec0:amd64.\n","Preparing to unpack .../12-libjbig2dec0_0.19-3build2_amd64.deb ...\n","Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n","Selecting previously unselected package libgs9:amd64.\n","Preparing to unpack .../13-libgs9_9.55.0~dfsg1-0ubuntu5.6_amd64.deb ...\n","Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.6) ...\n","Selecting previously unselected package ghostscript.\n","Preparing to unpack .../14-ghostscript_9.55.0~dfsg1-0ubuntu5.6_amd64.deb ...\n","Unpacking ghostscript (9.55.0~dfsg1-0ubuntu5.6) ...\n","Selecting previously unselected package gsfonts.\n","Preparing to unpack .../15-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.5_all.deb ...\n","Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.5) ...\n","Selecting previously unselected package imagemagick-6.q16.\n","Preparing to unpack .../16-imagemagick-6.q16_8%3a6.9.11.60+dfsg-1.3ubuntu0.22.04.3_amd64.deb ...\n","Unpacking imagemagick-6.q16 (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Selecting previously unselected package imagemagick.\n","Preparing to unpack .../17-imagemagick_8%3a6.9.11.60+dfsg-1.3ubuntu0.22.04.3_amd64.deb ...\n","Unpacking imagemagick (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Selecting previously unselected package libdjvulibre-text.\n","Preparing to unpack .../18-libdjvulibre-text_3.5.28-2build2_all.deb ...\n","Unpacking libdjvulibre-text (3.5.28-2build2) ...\n","Selecting previously unselected package libdjvulibre21:amd64.\n","Preparing to unpack .../19-libdjvulibre21_3.5.28-2build2_amd64.deb ...\n","Unpacking libdjvulibre21:amd64 (3.5.28-2build2) ...\n","Selecting previously unselected package libjxr0:amd64.\n","Preparing to unpack .../20-libjxr0_1.2~git20170615.f752187-5_amd64.deb ...\n","Unpacking libjxr0:amd64 (1.2~git20170615.f752187-5) ...\n","Selecting previously unselected package libjxr-tools.\n","Preparing to unpack .../21-libjxr-tools_1.2~git20170615.f752187-5_amd64.deb ...\n","Unpacking libjxr-tools (1.2~git20170615.f752187-5) ...\n","Selecting previously unselected package libwmflite-0.2-7:amd64.\n","Preparing to unpack .../22-libwmflite-0.2-7_0.2.12-5ubuntu1_amd64.deb ...\n","Unpacking libwmflite-0.2-7:amd64 (0.2.12-5ubuntu1) ...\n","Selecting previously unselected package libmagickcore-6.q16-6-extra:amd64.\n","Preparing to unpack .../23-libmagickcore-6.q16-6-extra_8%3a6.9.11.60+dfsg-1.3ubuntu0.22.04.3_amd64.deb ...\n","Unpacking libmagickcore-6.q16-6-extra:amd64 (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Selecting previously unselected package libnetpbm10.\n","Preparing to unpack .../24-libnetpbm10_2%3a10.0-15.4_amd64.deb ...\n","Unpacking libnetpbm10 (2:10.0-15.4) ...\n","Selecting previously unselected package netpbm.\n","Preparing to unpack .../25-netpbm_2%3a10.0-15.4_amd64.deb ...\n","Unpacking netpbm (2:10.0-15.4) ...\n","Setting up imagemagick-6-common (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Setting up fonts-noto-mono (20201225-1build1) ...\n","Setting up libwmflite-0.2-7:amd64 (0.2.12-5ubuntu1) ...\n","Setting up libijs-0.35:amd64 (0.35-15build2) ...\n","Setting up libjxr0:amd64 (1.2~git20170615.f752187-5) ...\n","Setting up libnetpbm10 (2:10.0-15.4) ...\n","Setting up fonts-urw-base35 (20200910-1) ...\n","Setting up poppler-data (0.4.11-1) ...\n","Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n","Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.5) ...\n","Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n","Setting up netpbm (2:10.0-15.4) ...\n","Setting up libfftw3-double3:amd64 (3.3.8-2ubuntu8) ...\n","Setting up liblqr-1-0:amd64 (0.4.2-2.1) ...\n","Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n","Setting up libdjvulibre-text (3.5.28-2build2) ...\n","Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.6) ...\n","Setting up libjxr-tools (1.2~git20170615.f752187-5) ...\n","Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.6) ...\n","Setting up libdjvulibre21:amd64 (3.5.28-2build2) ...\n","Setting up ghostscript (9.55.0~dfsg1-0ubuntu5.6) ...\n","Setting up libmagickcore-6.q16-6:amd64 (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Setting up libmagickwand-6.q16-6:amd64 (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Setting up libmagickcore-6.q16-6-extra:amd64 (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Setting up imagemagick-6.q16 (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","update-alternatives: using /usr/bin/compare-im6.q16 to provide /usr/bin/compare (compare) in auto mode\n","update-alternatives: using /usr/bin/compare-im6.q16 to provide /usr/bin/compare-im6 (compare-im6) in auto mode\n","update-alternatives: using /usr/bin/animate-im6.q16 to provide /usr/bin/animate (animate) in auto mode\n","update-alternatives: using /usr/bin/animate-im6.q16 to provide /usr/bin/animate-im6 (animate-im6) in auto mode\n","update-alternatives: using /usr/bin/convert-im6.q16 to provide /usr/bin/convert (convert) in auto mode\n","update-alternatives: using /usr/bin/convert-im6.q16 to provide /usr/bin/convert-im6 (convert-im6) in auto mode\n","update-alternatives: using /usr/bin/composite-im6.q16 to provide /usr/bin/composite (composite) in auto mode\n","update-alternatives: using /usr/bin/composite-im6.q16 to provide /usr/bin/composite-im6 (composite-im6) in auto mode\n","update-alternatives: using /usr/bin/conjure-im6.q16 to provide /usr/bin/conjure (conjure) in auto mode\n","update-alternatives: using /usr/bin/conjure-im6.q16 to provide /usr/bin/conjure-im6 (conjure-im6) in auto mode\n","update-alternatives: using /usr/bin/import-im6.q16 to provide /usr/bin/import (import) in auto mode\n","update-alternatives: using /usr/bin/import-im6.q16 to provide /usr/bin/import-im6 (import-im6) in auto mode\n","update-alternatives: using /usr/bin/identify-im6.q16 to provide /usr/bin/identify (identify) in auto mode\n","update-alternatives: using /usr/bin/identify-im6.q16 to provide /usr/bin/identify-im6 (identify-im6) in auto mode\n","update-alternatives: using /usr/bin/stream-im6.q16 to provide /usr/bin/stream (stream) in auto mode\n","update-alternatives: using /usr/bin/stream-im6.q16 to provide /usr/bin/stream-im6 (stream-im6) in auto mode\n","update-alternatives: using /usr/bin/display-im6.q16 to provide /usr/bin/display (display) in auto mode\n","update-alternatives: using /usr/bin/display-im6.q16 to provide /usr/bin/display-im6 (display-im6) in auto mode\n","update-alternatives: using /usr/bin/montage-im6.q16 to provide /usr/bin/montage (montage) in auto mode\n","update-alternatives: using /usr/bin/montage-im6.q16 to provide /usr/bin/montage-im6 (montage-im6) in auto mode\n","update-alternatives: using /usr/bin/mogrify-im6.q16 to provide /usr/bin/mogrify (mogrify) in auto mode\n","update-alternatives: using /usr/bin/mogrify-im6.q16 to provide /usr/bin/mogrify-im6 (mogrify-im6) in auto mode\n","Setting up imagemagick (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"]}],"source":["!apt install imagemagick"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":60749,"status":"ok","timestamp":1713902306998,"user":{"displayName":"Keya","userId":"10387939749915914234"},"user_tz":-330},"id":"UpCybGcBMfH1","outputId":"44bef0e5-2ae2-4140-a57e-d12fb33eda4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Render step 0/320\n","Render step 0/320\n","Render step 3/320\n","Render step 6/320\n","Render step 9/320\n","Render step 12/320\n","Render step 15/320\n","Render step 18/320\n","Render step 21/320\n","Render step 24/320\n","Render step 27/320\n","Render step 30/320\n","Render step 33/320\n","Render step 36/320\n","Render step 39/320\n","Render step 42/320\n","Render step 45/320\n","Render step 48/320\n","Render step 51/320\n","Render step 54/320\n","Render step 57/320\n","Render step 60/320\n","Render step 63/320\n","Render step 66/320\n","Render step 69/320\n","Render step 72/320\n","Render step 75/320\n","Render step 78/320\n","Render step 81/320\n","Render step 84/320\n","Render step 87/320\n","Render step 90/320\n","Render step 93/320\n","Render step 96/320\n","Render step 99/320\n","Render step 102/320\n","Render step 105/320\n","Render step 108/320\n","Render step 111/320\n","Render step 114/320\n","Render step 117/320\n","Render step 120/320\n","Render step 123/320\n","Render step 126/320\n","Render step 129/320\n","Render step 132/320\n","Render step 135/320\n","Render step 138/320\n","Render step 141/320\n","Render step 144/320\n","Render step 147/320\n","Render step 150/320\n","Render step 153/320\n","Render step 156/320\n","Render step 159/320\n","Render step 162/320\n","Render step 165/320\n","Render step 168/320\n","Render step 171/320\n","Render step 174/320\n","Render step 177/320\n","Render step 180/320\n","Render step 183/320\n","Render step 186/320\n","Render step 189/320\n","Render step 192/320\n","Render step 195/320\n","Render step 198/320\n","Render step 201/320\n","Render step 204/320\n","Render step 207/320\n","Render step 210/320\n","Render step 213/320\n","Render step 216/320\n","Render step 219/320\n","Render step 222/320\n","Render step 225/320\n","Render step 228/320\n","Render step 231/320\n","Render step 234/320\n","Render step 237/320\n","Render step 240/320\n","Render step 243/320\n","Render step 246/320\n","Render step 249/320\n","Render step 252/320\n","Render step 255/320\n","Render step 258/320\n","Render step 261/320\n","Render step 264/320\n","Render step 267/320\n","Render step 270/320\n","Render step 273/320\n","Render step 276/320\n","Render step 279/320\n","Render step 282/320\n","Render step 285/320\n","Render step 288/320\n","Render step 291/320\n","Render step 294/320\n","Render step 297/320\n","Render step 300/320\n","Render step 303/320\n","Render step 306/320\n","Render step 309/320\n","Render step 312/320\n","Render step 315/320\n","Render step 318/320\n","Animation saved to: /content/drive/MyDrive/rollouts-106500_num_1_timestamp_100/train/rollout_ex1.gif\n","Render step 0/320\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import pickle\n","from absl import app\n","from absl import flags\n","\n","from matplotlib import animation\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","from pyevtk.hl import pointsToVTK\n","\n","del_all_flags(flags.FLAGS)\n","\n","flags.DEFINE_string(\"rollout_dir\", \"/content/drive/MyDrive/rollouts-106500_num_1_timestamp_100/train/\", help=\"Directory where rollout.pkl are located\")\n","flags.DEFINE_string(\"rollout_name\", \"rollout_ex1\", help=\"Name of rollout `.pkl` file\")\n","flags.DEFINE_integer(\"step_stride\", 3, help=\"Stride of steps to skip.\")\n","flags.DEFINE_bool(\"change_yz\", False, help=\"Change y and z axis.\")\n","flags.DEFINE_enum(\"output_mode\", \"gif\", [\"gif\", \"vtk\"], help=\"Type of render output\")\n","\n","FLAGS = flags.FLAGS\n","FLAGS(sys.argv[1:])\n","\n","TYPE_TO_COLOR = {\n","    1: \"red\",  # for droplet\n","    3: \"black\",  # Boundary particles.\n","    0: \"green\",  # Rigid solids.\n","    7: \"magenta\",  # Goop.\n","    6: \"gold\",  # Sand.\n","    5: \"blue\",  # Water.\n","}\n","\n","\n","class Render():\n","    def __init__(self, input_dir, input_name):\n","        # Texts to describe rollout cases for data and render\n","        rollout_cases = [\n","            [\"ground_truth_rollout\", \"Reality\"], [\"predicted_rollout\", \"GNS\"]]\n","        self.rollout_cases = rollout_cases\n","        self.input_dir = input_dir\n","        self.input_name = input_name\n","        self.output_dir = input_dir\n","        self.output_name = input_name\n","\n","        # Get trajectory\n","        with open(f\"{self.input_dir}{self.input_name}.pkl\", \"rb\") as file:\n","            rollout_data = pickle.load(file)\n","        self.rollout_data = rollout_data\n","        trajectory = {}\n","        for rollout_case in rollout_cases:\n","            trajectory[rollout_case[0]] = np.concatenate(\n","                [rollout_data[\"initial_positions\"], rollout_data[rollout_case[0]]], axis=0\n","            )\n","        self.trajectory = trajectory\n","        self.loss = self.rollout_data['loss'].item()\n","\n","        # Trajectory information\n","        self.dims = trajectory[rollout_cases[0][0]].shape[2]\n","        self.num_particles = trajectory[rollout_cases[0][0]].shape[1]\n","        # self.num_steps = 100\n","        self.num_steps = trajectory[rollout_cases[0][0]].shape[0]\n","        self.boundaries = rollout_data[\"metadata\"][\"bounds\"]\n","        self.particle_type = rollout_data[\"particle_types\"]\n","\n","    def color_map(self):\n","        # color mask for visualization for different material types\n","        color_map = np.empty(self.num_particles, dtype=\"object\")\n","        for material_id, color in TYPE_TO_COLOR.items():\n","            print(material_id, color)\n","            color_index = np.where(np.array(self.particle_type) == material_id)\n","            print(color_index)\n","            color_map[color_index] = color\n","        color_map = list(color_map)\n","        return color_map\n","\n","    def color_mask(self):\n","        color_mask = []\n","        for material_id, color in TYPE_TO_COLOR.items():\n","            mask = np.array(self.particle_type) == material_id\n","            if mask.any() == True:\n","                color_mask.append([mask, color])\n","        return color_mask\n","\n","    def render_gif_animation(\n","            self, point_size=1, timestep_stride=3, vertical_camera_angle=20, viewpoint_rotation=0.5, change_yz=False\n","    ):\n","        # Init figures\n","        fig = plt.figure()\n","        if self.dims == 2:\n","            ax1 = fig.add_subplot(1, 2, 1, projection='rectilinear')\n","            ax2 = fig.add_subplot(1, 2, 2, projection='rectilinear')\n","            axes = [ax1, ax2]\n","        elif self.dims == 3:\n","            ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n","            ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n","            axes = [ax1, ax2]\n","\n","        # Define datacase name\n","        trajectory_datacases = [self.rollout_cases[0][0], self.rollout_cases[1][0]]\n","        render_datacases = [self.rollout_cases[0][1], self.rollout_cases[1][1]]\n","\n","        # Get boundary of simulation\n","        xboundary = self.boundaries[0]\n","        yboundary = self.boundaries[1]\n","        if self.dims == 3:\n","            zboundary = self.boundaries[2]\n","\n","        # Get color mask for visualization\n","        color_mask = self.color_mask()\n","\n","        # Fig creating function for 2d\n","        if self.dims == 2:\n","            def animate(i):\n","                print(f\"Render step {i}/{self.num_steps}\")\n","\n","                fig.clear()\n","                for j, datacase in enumerate(trajectory_datacases):\n","                    # select ax to plot at set boundary\n","                    axes[j] = fig.add_subplot(1, 2, j + 1, autoscale_on=False)\n","                    axes[j].set_aspect(\"equal\")\n","                    axes[j].set_xlim([float(xboundary[0]), float(xboundary[1])])\n","                    axes[j].set_ylim([float(yboundary[0]), float(yboundary[1])])\n","                    for mask, color in color_mask:\n","                        axes[j].scatter(self.trajectory[datacase][i][mask, 0],\n","                                        self.trajectory[datacase][i][mask, 1], s=point_size, color=color)\n","                    axes[j].grid(True, which='both')\n","                    axes[j].set_title(render_datacases[j])\n","                fig.suptitle(f\"{i}/{self.num_steps}, Total MSE: {self.loss:.2e}\")\n","\n","        # Fig creating function for 3d\n","        elif self.dims == 3:\n","\n","\n","        # Creat animation\n","        ani = animation.FuncAnimation(\n","            fig, animate, frames=np.arange(0, self.num_steps, timestep_stride), interval=10)\n","\n","        ani.save(f'{self.output_dir}{self.output_name}.gif', dpi=100, fps=30, writer='imagemagick')\n","        print(f\"Animation saved to: {self.output_dir}{self.output_name}.gif\")\n","\n","    def write_vtk(self):\n","        \"\"\"\n","        Write `.vtk` files for each timestep for each rollout case.\n","        \"\"\"\n","        for rollout_case, label in self.rollout_cases:\n","            path = f\"{self.output_dir}{self.output_name}_vtk-{label}\"\n","            if not os.path.exists(path):\n","                os.makedirs(path)\n","            initial_position = self.trajectory[rollout_case][0]\n","            for i, coord in enumerate(self.trajectory[rollout_case]):\n","                disp = np.linalg.norm(coord - initial_position, axis=1)\n","                pointsToVTK(f\"{path}/points{i}\",\n","                            np.array(coord[:, 0]),\n","                            np.array(coord[:, 1]),\n","                            np.zeros_like(coord[:, 1]) if self.dims == 2 else np.array(coord[:, 2]),\n","                            data={\"displacement\": disp})\n","        print(f\"vtk saved to: {self.output_dir}{self.output_name}...\")\n","\n","\n","def main():\n","    if not FLAGS.rollout_dir:\n","        raise ValueError(\"A `rollout_dir` must be passed.\")\n","    if not FLAGS.rollout_name:\n","        raise ValueError(\"A `rollout_name`must be passed.\")\n","\n","    render = Render(input_dir=FLAGS.rollout_dir, input_name=FLAGS.rollout_name)\n","\n","    if FLAGS.output_mode == \"gif\":\n","        render.render_gif_animation(\n","            point_size=1,\n","            timestep_stride=FLAGS.step_stride,\n","            vertical_camera_angle=20,\n","            viewpoint_rotation=0.3,\n","            change_yz=FLAGS.change_yz\n","        )\n","    elif FLAGS.output_mode == \"vtk\":\n","        render.write_vtk()\n","\n","main()"]},{"cell_type":"code","source":[],"metadata":{"id":"UkDMkssIzQQ4"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1wEk2-2XXvz4xxIKPNuuA3ndbzinvzzoh","timestamp":1713590336948}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}